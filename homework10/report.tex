\documentclass{article}

\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}

\begin{document}

\title{Relative Mistake Bound\\
       for Weighted-Majority}
\author{Geoffrey Ulman\\
        Homework 10\\
        CSI873}
\date{November 2011}
\maketitle

\section{Theorem}\label{Theorem}

\begin{equation}
\frac{-k \log _2 \left( \beta \right) + \log _2 \left( n \right)}{1 - \log _2 \left( 1 + \beta \right)}
\end{equation}

\section{Proof}\label{Proof}

The weight for a given prediction algorithm after making \(k\) mistakes is given by Equation \ref{eq1}.

\begin{equation}\label{eq1}
\beta^k
\end{equation}

Let \(w_0\) be the initial sum of the weights for all \(n\) algorithms given by Equation \ref{eq2}.

\begin{equation}\label{eq2}
\sum_{i=1}^{n} w_{j} = n = w_{0}
\end{equation}

In the worst case, all \(n\) prediction algorithms make a mistake and the total weight sum is given by Equation \ref{eq3}.

\begin{equation}\label{eq3}
\sum_{i=1}^{n} \beta w_{j} = \beta \sum_{i=1}^{n} w_{j} = \beta n
\end{equation}



\begin{thebibliography}{9}

\bibitem{cpl}
  Tom M. Mitchell,
  \emph{Machine Learning},
  WCB McGraw-Hill, Boston,
  1997.

\end{thebibliography}

\end{document}
