\documentclass{article}

\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}

\begin{document}

\title{K-Nearest Neighbors}
\author{Geoffrey Ulman\\
        Homework 11\\
        CSI873}
\date{December 2011}
\maketitle

\section{Results}\label{Results}

A testing error rate of \(0.388\) and a training error rate of \(0.322\) was achieved by the Naive Bayes classifier with 95\% confidence intervals given by Table \ref{error}. On a digit-by-digit basis, error was relatively uniform among the digits, with ``4'' and ``5'' being significant exceptions (see Figure \ref{trainconfusion} for the training data confusion matrix and Figure \ref{testconfusion} for the testing data confusion matrix). In the case of ``5'' misclassification were spread over a number of digits. In the case of ``4'' almost all the misclassification were misclassifying ``4'' as ``9'' (see Figures \ref{4_9_missclass_training} and \ref{4_9_missclass_testing}).

Java (version 1.6.0\_27) was used to implement the Naive Bayes classifier. As with the midterm project, the code is available as a Subversion repository on Google Code at \url{http://code.google.com/p/csi873/}. Compiling and running the code requires the Java build tool Maven (\url{http://maven.apache.org/}). Only the small amount of new code necessary to implement the Naive Bayes classifier is attached to this report.


\begin{table}
\caption{Uniform Weight Error}
\begin{center}
\begin{tabular}{llcc}
\toprule
K & Error & \multicolumn{2}{c}{95\% Confidence Interval} \\
\cmidrule(r){3-4}
& & Lower Bound & Upper Bound \\
\midrule
1 & 0.254 & 0.212 & 0.296 \\
2 & 0.266 & 0.223 & 0.309 \\
3 & 0.259 & 0.216 & 0.301 \\
4 & 0.241 & 0.200 & 0.283 \\
5 & 0.273 & 0.230 & 0.316 \\
6 & 0.280 & 0.237 & 0.324 \\
7 & 0.268 & 0.225 & 0.311 \\
\bottomrule
\end{tabular}
\label{error}
\end{center}
\end{table}

\begin{table}
\caption{Decaying Weight Error}
\begin{center}
\begin{tabular}{llcc}
\toprule
K & Error & \multicolumn{2}{c}{95\% Confidence Interval} \\
\cmidrule(r){3-4}
& & Lower Bound & Upper Bound \\
\midrule
1 & 0.254 & 0.212 & 0.296 \\
2 & 0.244 & 0.202 & 0.285 \\
3 & 0.227 & 0.186 & 0.267 \\
4 & 0.234 & 0.193 & 0.275 \\
5 & 0.237 & 0.195 & 0.278 \\
6 & 0.229 & 0.189 & 0.270 \\
7 & 0.217 & 0.177 & 0.257 \\
410 & 0.410 & 0.362 & 0.457 \\
\bottomrule
\end{tabular}
\label{error}
\end{center}
\end{table}


\begin{thebibliography}{9}

\bibitem{cpl}
  Tom M. Mitchell,
  \emph{Machine Learning},
  WCB McGraw-Hill, Boston,
  1997.

\end{thebibliography}

\end{document}
